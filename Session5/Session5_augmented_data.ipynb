{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                             review\n",
       "0  __label__4  The Rock is destined to be the 21st Century 's...\n",
       "1  __label__5  The gorgeously elaborate continuation of `` Th...\n",
       "2  __label__4  Singer/composer Bryan Adams contributes a slew...\n",
       "3  __label__3  You 'd think by now America would have had eno...\n",
       "4  __label__4               Yet the act is still charming here ."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv('fine-grained-sentiment/data/sst/sst_train.txt', header=None,names=['sentiment', 'review'],sep='\\t')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentiment'] = df_train['sentiment'].str.replace('__label__', '')\n",
    "df_train['sentiment'] = df_train['sentiment'].astype(int)\n",
    "\n",
    "df_test = pd.read_csv('fine-grained-sentiment/data/sst/sst_test.txt', sep='\\t', header=None,\n",
    "                   names=['sentiment', 'review'],\n",
    "                  )\n",
    "df_test['sentiment'] = df_test['sentiment'].str.replace('__label__', '')\n",
    "df_test['sentiment'] = df_test['sentiment'].astype(int)\n",
    "\n",
    "\n",
    "df_dev = pd.read_csv('fine-grained-sentiment/data/sst/sst_dev.txt', sep='\\t', header=None,\n",
    "                   names=['sentiment', 'review'],\n",
    "                  )\n",
    "df_dev['sentiment'] = df_dev['sentiment'].str.replace('__label__', '')\n",
    "df_dev['sentiment'] = df_dev['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8544, 2), (2210, 2), (1101, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape,df_test.shape,df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2322\n",
       "2    2218\n",
       "3    1624\n",
       "5    1288\n",
       "1    1092\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply augmentation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3139"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# random.randint(2,100)\n",
    "instances=[random.randint(1,8000) for i in range(4000) ]\n",
    "len(set(instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(words, p=0.3):\n",
    "    words_list=words.copy()\n",
    "    if len(words_list) ==1: # return if single word\n",
    "        return words_list\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words_list))\n",
    "    if len(remaining) == 0: # if not left, sample a random word\n",
    "        return [random.choice(words_list)] \n",
    "    else:\n",
    "        return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(sentence, n=5): \n",
    "    length = range(len(sentence)) \n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(length, 2)\n",
    "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_aug = pd.DataFrame(columns=['sentiment','review'])\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords = nltk_stopwords + ['-','.',',',\"'s\",'--',\"...\",\"\",\"'\",\"`\",\"(\",\")\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances=list(set(instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuda/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/anuda/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in instances:\n",
    "    temp=df_train.loc[i]\n",
    "    if random.randint(1,2)==1:\n",
    "        x_list =[x for x in random_deletion(tknzr.tokenize(temp['review'])) if x not in nltk_stopwords]\n",
    "        temp['review'] = temp['review']=(' ').join(x_list)\n",
    "    else:\n",
    "        x_list =[x for x in random_swap(tknzr.tokenize(temp['review'])) if x not in nltk_stopwords]\n",
    "        temp['review'] = temp['review']=(' ').join(x_list)\n",
    "    df_train_aug = df_train_aug.append(temp)\n",
    "# x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3139, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11683, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.append(df_train_aug)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch, torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5d30c99438>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=43\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- https://github.com/dsfsi/textaugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Field helps to do common data/text processing and converts to tensor:\n",
    "\n",
    "- Define the tokenizer, mention if the data is sequential in nature\n",
    "- batch_first : If set to true, will have the batch dimension first such as [1,1,28,28] First 1 is the batch size. Rest is size of the image\n",
    "- is_target: if this field is target variable\n",
    "- Stop_words can also be provided here\n",
    "- unknown and padding token can be explicity defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove punctuation\n",
    "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # remove newline\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuda/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/anuda/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "Review = data.Field(sequential = True, tokenize='spacy', batch_first=True,lower=True,\n",
    "                   include_lengths=True)\n",
    "Label = data.LabelField(sequential = True,tokenize='spacy', is_target=True,\n",
    "                        batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tuple of text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('review',Review),('label',Label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_dev.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['len'] = df_train.review.apply(lambda x: len(x))\n",
    "df_train.loc[df_train.len==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.len!=0]\n",
    "df_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a example object from list of reviews and sentiments from field tuple\n",
    "To access individual elements:\n",
    "\n",
    "example[0].review,example[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuda/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_example = [data.Example.fromlist([df_train.review[i],df_train.sentiment[i]], fields) for i in range(df_train.shape[0])]\n",
    "test_example = [data.Example.fromlist([df_test.review[i],df_test.sentiment[i]], fields) for i in range(df_test.shape[0])]\n",
    "valid_example = [data.Example.fromlist([df_dev.review[i],df_dev.sentiment[i]], fields) for i in range(df_dev.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'rock',\n",
       "  'is',\n",
       "  'destined',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  '21st',\n",
       "  'century',\n",
       "  \"'s\",\n",
       "  'new',\n",
       "  '``',\n",
       "  'conan',\n",
       "  \"''\",\n",
       "  'and',\n",
       "  'that',\n",
       "  'he',\n",
       "  \"'s\",\n",
       "  'going',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'splash',\n",
       "  'even',\n",
       "  'greater',\n",
       "  'than',\n",
       "  'arnold',\n",
       "  'schwarzenegger',\n",
       "  ',',\n",
       "  'jean',\n",
       "  '-',\n",
       "  'claud',\n",
       "  'van',\n",
       "  'damme',\n",
       "  'or',\n",
       "  'steven',\n",
       "  'segal',\n",
       "  '.'],\n",
       " 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_example[0].review,train_example[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Dataset = data.Dataset(train_example, fields)\n",
    "test_Dataset = data.Dataset(test_example, fields)\n",
    "valid_Dataset = data.Dataset(valid_example,fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 2210, 1101)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_Dataset), len(test_Dataset), len(valid_Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': ['what',\n",
       "  'really',\n",
       "  'surprises',\n",
       "  'about',\n",
       "  'wisegirls',\n",
       "  'is',\n",
       "  'its',\n",
       "  'low',\n",
       "  '-',\n",
       "  'key',\n",
       "  'quality',\n",
       "  'and',\n",
       "  'genuine',\n",
       "  'tenderness',\n",
       "  '.'],\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test_Dataset.examples[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review.build_vocab(train_Dataset, max_size=10000)\n",
    "Label.build_vocab(train_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input vocab :  10002\n",
      "Size of label vocab :  5\n",
      "Top 10 words appreared repeatedly : [('.', 8055), ('the', 7793), (',', 7131), ('a', 5579), ('and', 4544), ('of', 4482), ('-', 3577), ('to', 3063), ('it', 2634), ('is', 2574)]\n",
      "Labels :  defaultdict(None, {4: 0, 2: 1, 3: 2, 5: 3, 1: 4})\n"
     ]
    }
   ],
   "source": [
    "print('Size of input vocab : ', len(Review.vocab))\n",
    "print('Size of label vocab : ', len(Label.vocab))\n",
    "print('Top 10 words appreared repeatedly :', list(Review.vocab.freqs.most_common(10)))\n",
    "print('Labels : ', Label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuda/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_Dataset, test_Dataset), batch_size = 32, \n",
    "                                                            sort_key = lambda x: len(x.review),\n",
    "                                                            sort_within_batch=True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "with open('tokenizer.pkl', 'wb') as tokens: \n",
    "    pickle.dump(Review.vocab.stoi, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    # Define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        \n",
    "        super().__init__()          \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.encoder = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        # try using nn.GRU or nn.RNN here and compare their performances\n",
    "        # try bidirectional and compare their performances\n",
    "        \n",
    "        # Dense layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # text = [batch size, sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "    \n",
    "        # Hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs = self.fc(hidden)   \n",
    "        \n",
    "        # Final activation function softmax\n",
    "        output = F.softmax(dense_outputs[0], dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "size_of_vocab = len(Review.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 100\n",
    "num_output_nodes = 5\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(10002, 300)\n",
      "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "The model has 3,242,705 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    _, predictions = torch.max(preds, 1)\n",
    "    \n",
    "    correct = (predictions == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "# push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    # initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        # retrieve text and no. of words\n",
    "        review, review_lengths = batch.review   \n",
    "        \n",
    "        # convert to 1D tensor\n",
    "        predictions = model(review, review_lengths).squeeze()  \n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.label)        \n",
    "        \n",
    "        # compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label)   \n",
    "        \n",
    "        # backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        # loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            # retrieve text and no. of words\n",
    "            review, review_lengths = batch.review\n",
    "            \n",
    "            # convert to 1d tensor\n",
    "            predictions = model(review, review_lengths).squeeze()\n",
    "            \n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            # keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.301 | Train Acc: 59.88%\n",
      "\t Val. Loss: 1.587 |  Val. Acc: 29.96% \n",
      "\n",
      "\tTrain Loss: 1.279 | Train Acc: 62.30%\n",
      "\t Val. Loss: 1.588 |  Val. Acc: 30.00% \n",
      "\n",
      "\tTrain Loss: 1.264 | Train Acc: 63.75%\n",
      "\t Val. Loss: 1.589 |  Val. Acc: 29.73% \n",
      "\n",
      "\tTrain Loss: 1.254 | Train Acc: 64.89%\n",
      "\t Val. Loss: 1.589 |  Val. Acc: 29.73% \n",
      "\n",
      "\tTrain Loss: 1.247 | Train Acc: 65.65%\n",
      "\t Val. Loss: 1.590 |  Val. Acc: 29.69% \n",
      "\n",
      "\tTrain Loss: 1.241 | Train Acc: 66.17%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.64% \n",
      "\n",
      "\tTrain Loss: 1.236 | Train Acc: 66.73%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.73% \n",
      "\n",
      "\tTrain Loss: 1.232 | Train Acc: 67.09%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.91% \n",
      "\n",
      "\tTrain Loss: 1.228 | Train Acc: 67.50%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 29.91% \n",
      "\n",
      "\tTrain Loss: 1.224 | Train Acc: 67.90%\n",
      "\t Val. Loss: 1.592 |  Val. Acc: 29.60% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    # evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def predict(model, sentence):\n",
    "    tokenized = tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n",
    "    indexed = [Review.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
    "    length = [len(indexed)]                                    #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
    "    prediction = model(tensor, length_tensor)                  #prediction \n",
    "    return torch.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,df_test.loc[3]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Effective but too-tepid biopic\n",
      "Sentiment: 3\n",
      "Model Predictions:tensor(1, device='cuda:0')\n",
      "--------------------\n",
      "Review: If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(1, device='cuda:0')\n",
      "--------------------\n",
      "Review: The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "Sentiment: 3\n",
      "Model Predictions:tensor(2, device='cuda:0')\n",
      "--------------------\n",
      "Review: Offers that rare combination of entertainment and education .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: Steers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: But he somehow pulls it off .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(2, device='cuda:0')\n",
      "--------------------\n",
      "Review: Take Care of My Cat offers a refreshingly different slice of Asian cinema .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(2, device='cuda:0')\n",
      "--------------------\n",
      "Review: This is a film well worth seeing , talking and singing heads and all .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(3, device='cuda:0')\n",
      "--------------------\n",
      "Review: What really surprises about Wisegirls is its low-key quality and genuine tenderness .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: ( Wendigo is ) why we go to the cinema : to be fed through the eye , the heart , the mind .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: One of the greatest family-oriented , fantasy-adventure movies ever .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(3, device='cuda:0')\n",
      "--------------------\n",
      "Review: Ultimately , it ponders the reasons we need stories so much .\n",
      "Sentiment: 3\n",
      "Model Predictions:tensor(1, device='cuda:0')\n",
      "--------------------\n",
      "Review: An utterly compelling ` who wrote it ' in which the reputation of the most famous author who ever lived comes into question .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(1, device='cuda:0')\n",
      "--------------------\n",
      "Review: Illuminating if overly talky documentary .\n",
      "Sentiment: 3\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: A masterpiece four years in the making .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(2, device='cuda:0')\n",
      "--------------------\n",
      "Review: The movie 's ripe , enrapturing beauty will tempt those willing to probe its inscrutable mysteries .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: Offers a breath of the fresh air of true sophistication .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: A thoughtful , provocative , insistently humanizing film .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(3, device='cuda:0')\n",
      "--------------------\n",
      "Review: With a cast that includes some of the top actors working in independent film , Lovely & Amazing involves us because it is so incisive , so bleakly amusing about how we go about our lives .\n",
      "Sentiment: 5\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: A disturbing and frighteningly evocative assembly of imagery and hypnotic music composed by Philip Glass .\n",
      "Sentiment: 3\n",
      "Model Predictions:tensor(1, device='cuda:0')\n",
      "--------------------\n",
      "Review: Not for everyone , but for those with whom it will connect , it 's a nice departure from standard moviegoing fare .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(3, device='cuda:0')\n",
      "--------------------\n",
      "Review: Scores a few points for doing what it does with a dedicated and good-hearted professionalism .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n",
      "Review: Occasionally melodramatic , it 's also extremely effective .\n",
      "Sentiment: 4\n",
      "Model Predictions:tensor(0, device='cuda:0')\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print('Review: '+df_test.loc[i]['review'])\n",
    "    print('Sentiment: '+str(df_test.loc[i]['sentiment']))\n",
    "    print('Model Predictions:'+str(predict(model,df_test.loc[i]['review'])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
